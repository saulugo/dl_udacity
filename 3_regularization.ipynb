{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 28, 28) (100000,)\n",
      "Validation set (5000, 28, 28) (5000,)\n",
      "Test set (5000, 28, 28) (5000,)\n",
      "Final Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  final_test_dataset = save['final_test_dataset']\n",
    "  final_test_labels = save['final_test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "  print('Final Test set', final_test_dataset.shape, final_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 784) (100000, 10)\n",
      "Validation set (5000, 784) (5000, 10)\n",
      "Test set (5000, 784) (5000, 10)\n",
      "Final Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "final_test_dataset, final_test_labels = reformat(final_test_dataset, final_test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Final Test set', final_test_dataset.shape,final_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's introduce L2 regularization in the SGD logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "\n",
    "def log_reg(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: tensor containing the weights\n",
    "    biases: tensor containing the biases \n",
    "    ''' \n",
    "    return tf.add(tf.matmul(x,weights),biases)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.5\n",
    "num_steps = 3001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = log_reg(tf_train_dataset, weights, biases)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 15.902328\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 11.4%\n",
      "Minibatch loss at step 500: 1.574934\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1000: 1.194848\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1500: 1.137943\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 2000: 1.120408\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 2500: 0.848964\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 3000: 1.092439\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 77.8%\n",
      "Test accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.5\n",
    "num_steps = 3001\n",
    "l2 = 0.002\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = log_reg(tf_train_dataset, weights, biases)\n",
    "  loss = tf.reduce_mean(tf.add(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels),l2*tf.nn.l2_loss(weights)))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases + l2*tf.nn.l2_loss(weights))\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases + l2*tf.nn.l2_loss(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 23.309307\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 13.3%\n",
      "Minibatch loss at step 500: 2.975939\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 1000: 1.237951\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1500: 1.077300\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 0.825405\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2500: 0.643632\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.906068\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 87.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuronal Network Model Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First lets define a fucntion to represent the topology of our Neuronal Network:\n",
    "#Topology: Multilayer Perceptron, 1 hidden layer with 1024 neurons and RELU activation function.\n",
    "\n",
    "def mlp(x, weights, biases,l2=0):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    if(l2==0):\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "    else:\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['h1']))\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['out']))\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 316.791687012\n",
      "Minibatch accuracy: 10.15625\n",
      "Validation accuracy: 28.94\n",
      "Minibatch loss at step 500: 17.3185119629\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.82\n",
      "Minibatch loss at step 1000: 15.0953474045\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 78.84\n",
      "Minibatch loss at step 1500: 5.28393745422\n",
      "Minibatch accuracy: 82.8125\n",
      "Validation accuracy: 81.52\n",
      "Minibatch loss at step 2000: 2.11602020264\n",
      "Minibatch accuracy: 91.40625\n",
      "Validation accuracy: 80.76\n",
      "Minibatch loss at step 2500: 11.7564563751\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.98\n",
      "Minibatch loss at step 3000: 3.33054542542\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 81.0\n",
      "Test accuracy: 87.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmqGoOFJEmqGoOFJEmqmv8fo0vni0F2r6wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe768159b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network Model With L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "l2 = 0.0001\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases,l2)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases,l2))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases,l2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 302.390563965\n",
      "Minibatch accuracy: 14.0625\n",
      "Validation accuracy: 23.2\n",
      "Minibatch loss at step 500: 13.2263364792\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 80.14\n",
      "Minibatch loss at step 1000: 9.59216308594\n",
      "Minibatch accuracy: 85.9375\n",
      "Validation accuracy: 80.08\n",
      "Minibatch loss at step 1500: 10.9576950073\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 76.52\n",
      "Minibatch loss at step 2000: 5.75169181824\n",
      "Minibatch accuracy: 84.375\n",
      "Validation accuracy: 82.36\n",
      "Minibatch loss at step 2500: 4.61680936813\n",
      "Minibatch accuracy: 91.40625\n",
      "Validation accuracy: 81.96\n",
      "Minibatch loss at step 3000: 3.21753931046\n",
      "Minibatch accuracy: 81.25\n",
      "Validation accuracy: 82.36\n",
      "Test accuracy: 88.68\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69eb41af50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 10\n",
    "learning_rate = 0.5\n",
    "display_step = 1\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "l2 = 0.0001\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases,l2)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases,l2))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases,l2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 309.693603516\n",
      "Minibatch accuracy: 7.8125\n",
      "Validation accuracy: 21.08\n",
      "Minibatch loss at step 1: 1481.8145752\n",
      "Minibatch accuracy: 22.65625\n",
      "Validation accuracy: 45.34\n",
      "Minibatch loss at step 2: 763.462219238\n",
      "Minibatch accuracy: 60.9375\n",
      "Validation accuracy: 38.66\n",
      "Minibatch loss at step 3: 959.102722168\n",
      "Minibatch accuracy: 33.59375\n",
      "Validation accuracy: 43.4\n",
      "Minibatch loss at step 4: 958.191101074\n",
      "Minibatch accuracy: 44.53125\n",
      "Validation accuracy: 44.9\n",
      "Minibatch loss at step 5: 518.217285156\n",
      "Minibatch accuracy: 42.96875\n",
      "Validation accuracy: 55.6\n",
      "Minibatch loss at step 6: 226.894729614\n",
      "Minibatch accuracy: 55.46875\n",
      "Validation accuracy: 63.66\n",
      "Minibatch loss at step 7: 222.007843018\n",
      "Minibatch accuracy: 63.28125\n",
      "Validation accuracy: 67.46\n",
      "Minibatch loss at step 8: 258.156036377\n",
      "Minibatch accuracy: 64.84375\n",
      "Validation accuracy: 67.26\n",
      "Minibatch loss at step 9: 165.40032959\n",
      "Minibatch accuracy: 69.53125\n",
      "Validation accuracy: 73.82\n",
      "Test accuracy: 81.72\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niKTt/wPv4dpdt7yaFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69eefb0250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First lets define a fucntion to represent the topology of our Neuronal Network:\n",
    "#Topology: Multilayer Perceptron, 1 hidden layer with 1024 neurons and RELU activation function.\n",
    "\n",
    "def mlp2(x, weights, biases,l2=0,drop_out=\"N\",keep_prob=1):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    if(l2==0 and drop_out==\"N\"):\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "    if(l2>0 and drop_out==\"N\"):\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['h1']))\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['out']))\n",
    "    if(l2==0 and drop_out==\"Y\"):\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #now we shutdown some neurons outputs and scale the rest by a factor of 1 + (1-keep_prob) = 2-keep_prob\n",
    "        h1_layer = tf.nn.dropout(h1_layer,keep_prob)*(2-keep_prob) \n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "    if(l2>0 and drop_out==\"Y\"):\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #now we shutdown some neurons outputs and scale the rest by a factor of 1 + (1-keep_prob) = 2-keep_prob\n",
    "        h1_layer = tf.nn.dropout(h1_layer,keep_prob)*(2-keep_prob) \n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['h1']))\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['out']))\n",
    "    \n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "l2 = 0.0001\n",
    "keep_prob = 0.9 #this is 1 - dropout rate. So, if I want 0.2 dropout probability I use 0.8 of keep_prob\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp2(tf_train_dataset, weights, biases,l2,drop_out=\"Y\",keep_prob=keep_prob)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp2(tf_valid_dataset,weights,biases,l2=l2,drop_out=\"N\")) #only apply dropout during training\n",
    "    test_prediction = tf.nn.softmax(mlp2(tf_test_dataset,weights,biases,l2=l2,drop_out=\"N\")) #only apply dropout during training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 376.112426758\n",
      "Minibatch accuracy: 10.15625\n",
      "Validation accuracy: 31.0\n",
      "Minibatch loss at step 500: 15.837141037\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 80.72\n",
      "Minibatch loss at step 1000: 10.9723367691\n",
      "Minibatch accuracy: 85.15625\n",
      "Validation accuracy: 80.02\n",
      "Minibatch loss at step 1500: 10.4364070892\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 77.8\n",
      "Minibatch loss at step 2000: 7.08001041412\n",
      "Minibatch accuracy: 78.90625\n",
      "Validation accuracy: 81.8\n",
      "Minibatch loss at step 2500: 5.04674243927\n",
      "Minibatch accuracy: 85.15625\n",
      "Validation accuracy: 80.72\n",
      "Minibatch loss at step 3000: 2.61497545242\n",
      "Minibatch accuracy: 81.25\n",
      "Validation accuracy: 81.6\n",
      "Test accuracy: 88.26\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYCFJkmrGYCFJkmrGYCFJkmrm/wd+YGT3F4a0gAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69eeea6f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, 1 layers, no regularization, batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_p(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    #h1 layer z = XW + b\n",
    "    h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    #h1 layer activation function relu(z)\n",
    "    h1_layer = tf.nn.relu(h1_layer)\n",
    "    #output layer (no activation needed after output layer)\n",
    "    out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "\n",
    "train_subset = 10000\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "n_imput = image_size*image_size\n",
    "num_steps = 11\n",
    "display_step = 1\n",
    "learning_rate = 0.5\n",
    "n_hidden_1 = 256\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = multilayer_p(tf_train_dataset,weights,biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(multilayer_p(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(multilayer_p(tf_test_dataset,weights,biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 181.722061\n",
      "Training accuracy: 6.6%\n",
      "Validation accuracy: 36.6%\n",
      "Loss at step 1: 208.635986\n",
      "Training accuracy: 36.6%\n",
      "Validation accuracy: 36.5%\n",
      "Loss at step 2: 250.484406\n",
      "Training accuracy: 36.9%\n",
      "Validation accuracy: 40.2%\n",
      "Loss at step 3: 151.815582\n",
      "Training accuracy: 40.9%\n",
      "Validation accuracy: 49.8%\n",
      "Loss at step 4: 78.494614\n",
      "Training accuracy: 49.8%\n",
      "Validation accuracy: 65.0%\n",
      "Loss at step 5: 40.019680\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 71.5%\n",
      "Loss at step 6: 26.947613\n",
      "Training accuracy: 72.3%\n",
      "Validation accuracy: 73.5%\n",
      "Loss at step 7: 27.941019\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 8: 21.756683\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 75.8%\n",
      "Loss at step 9: 19.972481\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 76.1%\n",
      "Loss at step 10: 18.750132\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 76.1%\n",
      "Test accuracy: 84.2%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % display_step == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "        # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "        # just to get that one numpy array. Note that it recomputes all its graph\n",
    "        # dependencies.\n",
    "        print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second try, 2 layers, no regularization, mini-batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_p2(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    #h1 layer z = XW + b\n",
    "    h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    #h1 layer activation function relu(z)\n",
    "    h1_layer = tf.nn.relu(h1_layer)\n",
    "    #output layer (no activation needed after output layer)\n",
    "    \n",
    "    h2_layer = tf.add(tf.matmul(h1_layer,weights['h2']), biases['b2'])\n",
    "    h2_layer = tf.nn.relu(h2_layer)\n",
    "    \n",
    "    out_layer = tf.add(tf.matmul(h2_layer,weights['out']), biases['out'])\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "\n",
    "batch_size = 128\n",
    "train_subset = 10000\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "n_imput = image_size*image_size\n",
    "num_steps = 3001\n",
    "display_step = 100\n",
    "learning_rate = 0.001\n",
    "n_hidden_1 = 1024\n",
    "n_hidden_2 = 512\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    \n",
    "    #tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    #tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.zeros([num_labels]))\n",
    "    }\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = multilayer_p2(tf_train_dataset,weights,biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "     \n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "       \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(multilayer_p2(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(multilayer_p2(tf_test_dataset,weights,biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 5192.82861328\n",
      "Minibatch accuracy: 10.9375\n",
      "Validation accuracy: 10.76\n",
      "Minibatch loss at step 100: 628.991882324\n",
      "Minibatch accuracy: 64.84375\n",
      "Validation accuracy: 71.0\n",
      "Minibatch loss at step 200: 219.382751465\n",
      "Minibatch accuracy: 82.8125\n",
      "Validation accuracy: 74.36\n",
      "Minibatch loss at step 300: 393.892700195\n",
      "Minibatch accuracy: 78.125\n",
      "Validation accuracy: 74.8\n",
      "Minibatch loss at step 400: 359.992706299\n",
      "Minibatch accuracy: 78.90625\n",
      "Validation accuracy: 76.2\n",
      "Minibatch loss at step 500: 358.429748535\n",
      "Minibatch accuracy: 73.4375\n",
      "Validation accuracy: 76.8\n",
      "Minibatch loss at step 600: 365.354095459\n",
      "Minibatch accuracy: 78.125\n",
      "Validation accuracy: 77.36\n",
      "Minibatch loss at step 700: 304.363555908\n",
      "Minibatch accuracy: 75.78125\n",
      "Validation accuracy: 77.48\n",
      "Minibatch loss at step 800: 405.116394043\n",
      "Minibatch accuracy: 71.875\n",
      "Validation accuracy: 76.56\n",
      "Minibatch loss at step 900: 199.693603516\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 77.84\n",
      "Minibatch loss at step 1000: 200.181152344\n",
      "Minibatch accuracy: 85.9375\n",
      "Validation accuracy: 78.48\n",
      "Minibatch loss at step 1100: 244.636413574\n",
      "Minibatch accuracy: 72.65625\n",
      "Validation accuracy: 78.16\n",
      "Minibatch loss at step 1200: 210.028030396\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 78.08\n",
      "Minibatch loss at step 1300: 170.63848877\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 78.24\n",
      "Minibatch loss at step 1400: 266.135375977\n",
      "Minibatch accuracy: 73.4375\n",
      "Validation accuracy: 78.62\n",
      "Minibatch loss at step 1500: 152.16394043\n",
      "Minibatch accuracy: 81.25\n",
      "Validation accuracy: 78.96\n",
      "Minibatch loss at step 1600: 159.460479736\n",
      "Minibatch accuracy: 81.25\n",
      "Validation accuracy: 79.1\n",
      "Minibatch loss at step 1700: 177.00100708\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.36\n",
      "Minibatch loss at step 1800: 140.812515259\n",
      "Minibatch accuracy: 82.8125\n",
      "Validation accuracy: 78.48\n",
      "Minibatch loss at step 1900: 91.0290374756\n",
      "Minibatch accuracy: 91.40625\n",
      "Validation accuracy: 79.3\n",
      "Minibatch loss at step 2000: 157.810409546\n",
      "Minibatch accuracy: 76.5625\n",
      "Validation accuracy: 78.88\n",
      "Minibatch loss at step 2100: 194.42288208\n",
      "Minibatch accuracy: 84.375\n",
      "Validation accuracy: 79.0\n",
      "Minibatch loss at step 2200: 196.511260986\n",
      "Minibatch accuracy: 79.6875\n",
      "Validation accuracy: 79.14\n",
      "Minibatch loss at step 2300: 132.992324829\n",
      "Minibatch accuracy: 83.59375\n",
      "Validation accuracy: 78.9\n",
      "Minibatch loss at step 2400: 126.647880554\n",
      "Minibatch accuracy: 83.59375\n",
      "Validation accuracy: 79.28\n",
      "Minibatch loss at step 2500: 126.765060425\n",
      "Minibatch accuracy: 85.15625\n",
      "Validation accuracy: 79.34\n",
      "Minibatch loss at step 2600: 119.522445679\n",
      "Minibatch accuracy: 85.15625\n",
      "Validation accuracy: 79.5\n",
      "Minibatch loss at step 2700: 129.654724121\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.08\n",
      "Minibatch loss at step 2800: 64.4269714355\n",
      "Minibatch accuracy: 85.9375\n",
      "Validation accuracy: 79.38\n",
      "Minibatch loss at step 2900: 123.272064209\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.54\n",
      "Minibatch loss at step 3000: 123.374290466\n",
      "Minibatch accuracy: 83.59375\n",
      "Validation accuracy: 79.44\n",
      "Test accuracy: 87.12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFkCAYAAACAUFlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f122752ebd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        \n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        #_, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        \n",
    "        if (step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third try, same topology, adding learning decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_p2(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    #h1 layer z = XW + b\n",
    "    h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    #h1 layer activation function relu(z)\n",
    "    h1_layer = tf.nn.relu(h1_layer)\n",
    "    #output layer (no activation needed after output layer)\n",
    "    \n",
    "    h2_layer = tf.add(tf.matmul(h1_layer,weights['h2']), biases['b2'])\n",
    "    h2_layer = tf.nn.relu(h2_layer)\n",
    "    \n",
    "    out_layer = tf.add(tf.matmul(h2_layer,weights['out']), biases['out'])\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "\n",
    "batch_size = 128\n",
    "train_subset = 10000\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "n_imput = image_size*image_size\n",
    "num_steps = 3001\n",
    "display_step = 100\n",
    "#initial_learning_rate = 0.001\n",
    "initial_learning_rate = 0.002\n",
    "n_hidden_1 = 1024\n",
    "n_hidden_2 = 512\n",
    "decay_steps = 100\n",
    "decay_rate = 0.98\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    \n",
    "    #tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    #tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    #graph variables\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int32)\n",
    "    increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,global_step,decay_steps=decay_steps, decay_rate=decay_rate)\n",
    "    \n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.zeros([num_labels]))\n",
    "    }\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = multilayer_p2(tf_train_dataset,weights,biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "     \n",
    "\n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=increment_global_step_op)\n",
    "       \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(multilayer_p2(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(multilayer_p2(tf_test_dataset,weights,biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4752.72705078\n",
      "Minibatch learning rate 0.00199959613383\n",
      "Minibatch accuracy: 10.9375\n",
      "Validation accuracy: 20.9\n",
      "Minibatch loss at step 100: 574.153076172\n",
      "Minibatch learning rate 0.00192041217815\n",
      "Minibatch accuracy: 64.84375\n",
      "Validation accuracy: 73.1\n",
      "Minibatch loss at step 200: 287.211914062\n",
      "Minibatch learning rate 0.00184436386917\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 75.56\n",
      "Minibatch loss at step 300: 391.690948486\n",
      "Minibatch learning rate 0.00177132710814\n",
      "Minibatch accuracy: 75.78125\n",
      "Validation accuracy: 77.14\n",
      "Minibatch loss at step 400: 282.073577881\n",
      "Minibatch learning rate 0.00170118268579\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 77.72\n",
      "Minibatch loss at step 500: 378.047912598\n",
      "Minibatch learning rate 0.0016338158166\n",
      "Minibatch accuracy: 76.5625\n",
      "Validation accuracy: 78.42\n",
      "Minibatch loss at step 600: 233.945068359\n",
      "Minibatch learning rate 0.00156911695376\n",
      "Minibatch accuracy: 79.6875\n",
      "Validation accuracy: 78.46\n",
      "Minibatch loss at step 700: 201.001068115\n",
      "Minibatch learning rate 0.00150697992649\n",
      "Minibatch accuracy: 77.34375\n",
      "Validation accuracy: 78.62\n",
      "Minibatch loss at step 800: 231.976669312\n",
      "Minibatch learning rate 0.00144730345346\n",
      "Minibatch accuracy: 78.125\n",
      "Validation accuracy: 78.16\n",
      "Minibatch loss at step 900: 185.868469238\n",
      "Minibatch learning rate 0.0013899904443\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 78.52\n",
      "Minibatch loss at step 1000: 192.520172119\n",
      "Minibatch learning rate 0.00133494683541\n",
      "Minibatch accuracy: 88.28125\n",
      "Validation accuracy: 78.88\n",
      "Minibatch loss at step 1100: 131.050582886\n",
      "Minibatch learning rate 0.00128208298702\n",
      "Minibatch accuracy: 78.125\n",
      "Validation accuracy: 79.24\n",
      "Minibatch loss at step 1200: 105.917022705\n",
      "Minibatch learning rate 0.00123131251894\n",
      "Minibatch accuracy: 86.71875\n",
      "Validation accuracy: 79.34\n",
      "Minibatch loss at step 1300: 157.535675049\n",
      "Minibatch learning rate 0.00118255265988\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 78.82\n",
      "Minibatch loss at step 1400: 195.325332642\n",
      "Minibatch learning rate 0.00113572354894\n",
      "Minibatch accuracy: 75.0\n",
      "Validation accuracy: 79.18\n",
      "Minibatch loss at step 1500: 132.8253479\n",
      "Minibatch learning rate 0.00109074893408\n",
      "Minibatch accuracy: 83.59375\n",
      "Validation accuracy: 78.86\n",
      "Minibatch loss at step 1600: 118.200378418\n",
      "Minibatch learning rate 0.00104755535722\n",
      "Minibatch accuracy: 84.375\n",
      "Validation accuracy: 79.2\n",
      "Minibatch loss at step 1700: 128.996795654\n",
      "Minibatch learning rate 0.00100607215427\n",
      "Minibatch accuracy: 79.6875\n",
      "Validation accuracy: 79.32\n",
      "Minibatch loss at step 1800: 98.8738021851\n",
      "Minibatch learning rate 0.000966231804341\n",
      "Minibatch accuracy: 85.9375\n",
      "Validation accuracy: 79.32\n",
      "Minibatch loss at step 1900: 103.777267456\n",
      "Minibatch learning rate 0.00092796899844\n",
      "Minibatch accuracy: 84.375\n",
      "Validation accuracy: 79.58\n",
      "Minibatch loss at step 2000: 154.462478638\n",
      "Minibatch learning rate 0.000891221512575\n",
      "Minibatch accuracy: 77.34375\n",
      "Validation accuracy: 79.52\n",
      "Minibatch loss at step 2100: 155.313598633\n",
      "Minibatch learning rate 0.000855929160025\n",
      "Minibatch accuracy: 84.375\n",
      "Validation accuracy: 79.66\n",
      "Minibatch loss at step 2200: 154.696731567\n",
      "Minibatch learning rate 0.000822034373414\n",
      "Minibatch accuracy: 78.90625\n",
      "Validation accuracy: 79.94\n",
      "Minibatch loss at step 2300: 110.479286194\n",
      "Minibatch learning rate 0.000789481855463\n",
      "Minibatch accuracy: 85.9375\n",
      "Validation accuracy: 79.68\n",
      "Minibatch loss at step 2400: 101.828636169\n",
      "Minibatch learning rate 0.000758218462579\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 79.32\n",
      "Minibatch loss at step 2500: 92.458480835\n",
      "Minibatch learning rate 0.000728193030227\n",
      "Minibatch accuracy: 86.71875\n",
      "Validation accuracy: 79.5\n",
      "Minibatch loss at step 2600: 110.494293213\n",
      "Minibatch learning rate 0.000699356605764\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.68\n",
      "Minibatch loss at step 2700: 100.024604797\n",
      "Minibatch learning rate 0.000671662099194\n",
      "Minibatch accuracy: 86.71875\n",
      "Validation accuracy: 79.68\n",
      "Minibatch loss at step 2800: 74.7847290039\n",
      "Minibatch learning rate 0.000645064283162\n",
      "Minibatch accuracy: 86.71875\n",
      "Validation accuracy: 79.52\n",
      "Minibatch loss at step 2900: 110.268737793\n",
      "Minibatch learning rate 0.000619519734755\n",
      "Minibatch accuracy: 75.78125\n",
      "Validation accuracy: 79.6\n",
      "Minibatch loss at step 3000: 98.6643066406\n",
      "Minibatch learning rate 0.000594986835495\n",
      "Minibatch accuracy: 79.6875\n",
      "Validation accuracy: 79.66\n",
      "Test accuracy: 87.66\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFkCAYAAACAUFlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWxk+JElSWxk+JElSWxk+JElSW/1/6e/T0V+MGbcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f122539cd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "                \n",
    "        #learning_rate = tf.train.exponential_decay(initial_learning_rate,global_step,decay_steps=decay_steps, decay_rate=decay_rate)\n",
    "        \n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_labels : batch_labels}\n",
    "        \n",
    "        _, l, predictions,_,lr = session.run([optimizer, loss, train_prediction, increment_global_step_op,learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        if (step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch learning rate {}\".format(lr))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Try, More layers, more neurons, learning decay rate (using Adam Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_p3(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    #h1 layer z = XW + b\n",
    "    h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    #h1 layer activation function relu(z)\n",
    "    h1_layer = tf.nn.relu(h1_layer)\n",
    "    #output layer (no activation needed after output layer)\n",
    "    \n",
    "    h2_layer = tf.add(tf.matmul(h1_layer,weights['h2']), biases['b2'])\n",
    "    h2_layer = tf.nn.relu(h2_layer)\n",
    "    \n",
    "    h3_layer = tf.add(tf.matmul(h2_layer,weights['h3']), biases['b3'])\n",
    "    h3_layer = tf.nn.relu(h3_layer)\n",
    "    \n",
    "    out_layer = tf.add(tf.matmul(h3_layer,weights['out']), biases['out'])\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "\n",
    "batch_size = 512\n",
    "train_subset = 10000\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "n_imput = image_size*image_size\n",
    "num_steps = 5001\n",
    "display_step = 100\n",
    "learning_rate = 0.001\n",
    "n_hidden_1 = 1024\n",
    "n_hidden_2 = 512\n",
    "n_hidden_3 = 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2])),\n",
    "        'h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_3,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'b3': tf.Variable(tf.zeros([n_hidden_3])),\n",
    "        'out': tf.Variable(tf.zeros([num_labels]))\n",
    "    }\n",
    "\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = multilayer_p3(tf_train_dataset,weights,biases)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "     \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "       \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(multilayer_p3(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(multilayer_p3(tf_test_dataset,weights,biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 56617.125\n",
      "Minibatch accuracy: 10.546875\n",
      "Validation accuracy: 12.2\n",
      "Minibatch loss at step 100: 3713.67407227\n",
      "Minibatch accuracy: 76.3671875\n",
      "Validation accuracy: 74.44\n",
      "Minibatch loss at step 200: 2829.4543457\n",
      "Minibatch accuracy: 76.3671875\n",
      "Validation accuracy: 76.78\n",
      "Minibatch loss at step 300: 2282.42138672\n",
      "Minibatch accuracy: 76.171875\n",
      "Validation accuracy: 78.06\n",
      "Minibatch loss at step 400: 1807.83361816\n",
      "Minibatch accuracy: 80.078125\n",
      "Validation accuracy: 77.66\n",
      "Minibatch loss at step 500: 1333.15917969\n",
      "Minibatch accuracy: 80.859375\n",
      "Validation accuracy: 78.9\n",
      "Minibatch loss at step 600: 1006.53112793\n",
      "Minibatch accuracy: 81.640625\n",
      "Validation accuracy: 78.58\n",
      "Minibatch loss at step 700: 888.672912598\n",
      "Minibatch accuracy: 83.3984375\n",
      "Validation accuracy: 79.48\n",
      "Minibatch loss at step 800: 1028.84545898\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 79.44\n",
      "Minibatch loss at step 900: 674.090881348\n",
      "Minibatch accuracy: 85.546875\n",
      "Validation accuracy: 79.68\n",
      "Minibatch loss at step 1000: 668.372070312\n",
      "Minibatch accuracy: 87.109375\n",
      "Validation accuracy: 79.56\n",
      "Minibatch loss at step 1100: 470.986968994\n",
      "Minibatch accuracy: 89.0625\n",
      "Validation accuracy: 80.56\n",
      "Minibatch loss at step 1200: 555.830993652\n",
      "Minibatch accuracy: 86.9140625\n",
      "Validation accuracy: 80.08\n",
      "Minibatch loss at step 1300: 383.563720703\n",
      "Minibatch accuracy: 85.546875\n",
      "Validation accuracy: 80.88\n",
      "Minibatch loss at step 1400: 366.327453613\n",
      "Minibatch accuracy: 88.8671875\n",
      "Validation accuracy: 80.72\n",
      "Minibatch loss at step 1500: 512.695678711\n",
      "Minibatch accuracy: 85.15625\n",
      "Validation accuracy: 80.98\n",
      "Minibatch loss at step 1600: 346.960083008\n",
      "Minibatch accuracy: 86.5234375\n",
      "Validation accuracy: 80.78\n",
      "Minibatch loss at step 1700: 265.849609375\n",
      "Minibatch accuracy: 88.8671875\n",
      "Validation accuracy: 80.88\n",
      "Minibatch loss at step 1800: 312.16607666\n",
      "Minibatch accuracy: 87.5\n",
      "Validation accuracy: 80.7\n",
      "Minibatch loss at step 1900: 187.617553711\n",
      "Minibatch accuracy: 90.625\n",
      "Validation accuracy: 81.2\n",
      "Minibatch loss at step 2000: 250.880859375\n",
      "Minibatch accuracy: 90.0390625\n",
      "Validation accuracy: 80.94\n",
      "Minibatch loss at step 2100: 254.267150879\n",
      "Minibatch accuracy: 88.4765625\n",
      "Validation accuracy: 81.22\n",
      "Minibatch loss at step 2200: 236.152023315\n",
      "Minibatch accuracy: 89.6484375\n",
      "Validation accuracy: 80.78\n",
      "Minibatch loss at step 2300: 173.848999023\n",
      "Minibatch accuracy: 90.4296875\n",
      "Validation accuracy: 81.68\n",
      "Minibatch loss at step 2400: 180.035339355\n",
      "Minibatch accuracy: 91.2109375\n",
      "Validation accuracy: 81.16\n",
      "Minibatch loss at step 2500: 155.33114624\n",
      "Minibatch accuracy: 91.6015625\n",
      "Validation accuracy: 81.44\n",
      "Minibatch loss at step 2600: 92.1562271118\n",
      "Minibatch accuracy: 92.7734375\n",
      "Validation accuracy: 81.42\n",
      "Minibatch loss at step 2700: 128.972732544\n",
      "Minibatch accuracy: 90.8203125\n",
      "Validation accuracy: 81.48\n",
      "Minibatch loss at step 2800: 120.494781494\n",
      "Minibatch accuracy: 92.3828125\n",
      "Validation accuracy: 81.74\n",
      "Minibatch loss at step 2900: 50.4909820557\n",
      "Minibatch accuracy: 93.5546875\n",
      "Validation accuracy: 81.64\n",
      "Minibatch loss at step 3000: 103.254196167\n",
      "Minibatch accuracy: 92.3828125\n",
      "Validation accuracy: 81.8\n",
      "Minibatch loss at step 3100: 64.3772964478\n",
      "Minibatch accuracy: 94.53125\n",
      "Validation accuracy: 82.08\n",
      "Minibatch loss at step 3200: 94.6942749023\n",
      "Minibatch accuracy: 95.5078125\n",
      "Validation accuracy: 81.86\n",
      "Minibatch loss at step 3300: 112.295715332\n",
      "Minibatch accuracy: 93.75\n",
      "Validation accuracy: 81.76\n",
      "Minibatch loss at step 3400: 71.515045166\n",
      "Minibatch accuracy: 92.3828125\n",
      "Validation accuracy: 82.0\n",
      "Minibatch loss at step 3500: 69.8883361816\n",
      "Minibatch accuracy: 93.9453125\n",
      "Validation accuracy: 82.36\n",
      "Minibatch loss at step 3600: 45.3486976624\n",
      "Minibatch accuracy: 95.3125\n",
      "Validation accuracy: 82.16\n",
      "Minibatch loss at step 3700: 56.0685577393\n",
      "Minibatch accuracy: 93.9453125\n",
      "Validation accuracy: 82.52\n",
      "Minibatch loss at step 3800: 52.2980117798\n",
      "Minibatch accuracy: 95.8984375\n",
      "Validation accuracy: 82.44\n",
      "Minibatch loss at step 3900: 19.2384204865\n",
      "Minibatch accuracy: 96.09375\n",
      "Validation accuracy: 82.08\n",
      "Minibatch loss at step 4000: 15.8738651276\n",
      "Minibatch accuracy: 97.4609375\n",
      "Validation accuracy: 82.56\n",
      "Minibatch loss at step 4100: 30.0364818573\n",
      "Minibatch accuracy: 96.484375\n",
      "Validation accuracy: 82.18\n",
      "Minibatch loss at step 4200: 46.796005249\n",
      "Minibatch accuracy: 96.09375\n",
      "Validation accuracy: 82.28\n",
      "Minibatch loss at step 4300: 25.2579441071\n",
      "Minibatch accuracy: 96.2890625\n",
      "Validation accuracy: 82.24\n",
      "Minibatch loss at step 4400: 33.6727409363\n",
      "Minibatch accuracy: 95.8984375\n",
      "Validation accuracy: 82.38\n",
      "Minibatch loss at step 4500: 36.0313262939\n",
      "Minibatch accuracy: 96.875\n",
      "Validation accuracy: 82.62\n",
      "Minibatch loss at step 4600: 22.3281230927\n",
      "Minibatch accuracy: 97.8515625\n",
      "Validation accuracy: 82.76\n",
      "Minibatch loss at step 4700: 9.40438270569\n",
      "Minibatch accuracy: 98.2421875\n",
      "Validation accuracy: 82.72\n",
      "Minibatch loss at step 4800: 21.4355354309\n",
      "Minibatch accuracy: 97.4609375\n",
      "Validation accuracy: 82.76\n",
      "Minibatch loss at step 4900: 13.5496273041\n",
      "Minibatch accuracy: 97.65625\n",
      "Validation accuracy: 82.74\n",
      "Minibatch loss at step 5000: 40.554019928\n",
      "Minibatch accuracy: 95.5078125\n",
      "Validation accuracy: 82.88\n",
      "Test accuracy: 90.54\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAFkCAYAAAD7dJuCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPzdLcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f122c139e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        \n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        #_, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        \n",
    "        if (step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 28, 28) (100000,)\n",
      "Validation set (5000, 28, 28) (5000,)\n",
      "Test set (5000, 28, 28) (5000,)\n",
      "Final Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  final_test_dataset = save['final_test_dataset']\n",
    "  final_test_labels = save['final_test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "  print('Final Test set', final_test_dataset.shape, final_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 784) (100000, 10)\n",
      "Validation set (5000, 784) (5000, 10)\n",
      "Test set (5000, 784) (5000, 10)\n",
      "Final Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "final_test_dataset, final_test_labels = reformat(final_test_dataset, final_test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Final Test set', final_test_dataset.shape,final_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's introduce L2 regularization in the SGD logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "\n",
    "def log_reg(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: tensor containing the weights\n",
    "    biases: tensor containing the biases \n",
    "    ''' \n",
    "    return tf.add(tf.matmul(x,weights),biases)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.5\n",
    "num_steps = 3001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = log_reg(tf_train_dataset, weights, biases)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.200386\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 500: 1.715070\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 1000: 1.680272\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1500: 0.815077\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 2000: 1.050925\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 2500: 1.091488\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 3000: 0.988056\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.0%\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.5\n",
    "num_steps = 3001\n",
    "l2 = 0.002\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = log_reg(tf_train_dataset, weights, biases)\n",
    "  loss = tf.reduce_mean(tf.add(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels),l2*tf.nn.l2_loss(weights)))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.158524\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 8.7%\n",
      "Minibatch loss at step 500: 2.680648\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1000: 1.424516\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1500: 0.787888\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 0.765174\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2500: 0.798362\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 3000: 0.717879\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuronal Network Model Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First lets define a fucntion to represent the topology of our Neuronal Network:\n",
    "#Topology: Multilayer Perceptron, 1 hidden layer with 1024 neurons and RELU activation function.\n",
    "\n",
    "def mlp(x, weights, biases,l2=0):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    if(l2==0):\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "    else:\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['h1']))\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['out']))\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 316.791687012\n",
      "Minibatch accuracy: 10.15625\n",
      "Validation accuracy: 28.94\n",
      "Minibatch loss at step 500: 17.3185119629\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.82\n",
      "Minibatch loss at step 1000: 15.0953474045\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 78.84\n",
      "Minibatch loss at step 1500: 5.28393745422\n",
      "Minibatch accuracy: 82.8125\n",
      "Validation accuracy: 81.52\n",
      "Minibatch loss at step 2000: 2.11602020264\n",
      "Minibatch accuracy: 91.40625\n",
      "Validation accuracy: 80.76\n",
      "Minibatch loss at step 2500: 11.7564563751\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.98\n",
      "Minibatch loss at step 3000: 3.33054542542\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 81.0\n",
      "Test accuracy: 87.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmUnXWd7/v3N/MAFEPIAAmTiEQmqWKexwRE6XZ5l20J\nrfawbnuO1+XNXX319Ll2263ddoun1T7a9PW0fbUBrXNsvdpcExJmEBmtQgyzCAgBEhIIRUggU/3u\nH79dp3aKmnbV3vXs4f1a61n128/z7Gd/9y97V33yDL8nUkpIkiRVw5SiC5AkSc3DYCFJkqrGYCFJ\nkqrGYCFJkqrGYCFJkqrGYCFJkqrGYCFJkqrGYCFJkqrGYCFJkqrGYCFJkqqmomAREZ+IiIciorc0\n3R0Rl5Qtvz0i+sqm3RFx9aBtLImIlRGxNSLWR8RVEWHAkSSpCUyrcP3ngc8CT5Uefxz494h4T0rp\nMSAB/w34cyBK62zrf3IpQKwCXgROAw4CrgV2AJ8b31uQJEn1IiZ6E7KIeAX405TSdyLiNuDBlNL/\nMcy6lwLXA4tSSptK8/4E+DvgwJTSrgkVI0mSCjXuQxARMSUiPgzMAe4uW3RFRGyMiLUR8aWImF22\n7DRgbX+oKFkDtAHHjLcWSZJUHyo9FEJEHAvcA8wCtgAfSCk9UVr8PeC35EMdxwNXAUcB/0tp+UJg\nw6BNbihb9tAwr3kAsBx4Fnir0polSWphs4DDgDUppVdq/WIVBwvgceAEYF/gg8A1EXFOSunxlNK3\ny9Z7JCLWA7dExOEppWdG2e5Ix2SWk0OLJEkanyuA79f6RSoOFqXzIJ4uPeyJiFOATwP/YYjV7yv9\nPBJ4BlgPnDxonQWln4P3ZJR7FuC6665j6dKllZbcslasWMHXvva1ostoOPZb5eyz8bHfKmefVe6x\nxx7jyiuvhNLf0lobzx6LwaYAM4dZdiJ5T8RLpcf3AP85IuaVnWexDOgFHh3hNd4CWLp0Ke3t7ROv\nuEW0tbXZX+Ngv1XOPhsf+61y9tmETMqpBBUFi4j4G+AG8mWne5N3q5wLLIuII4CPkC8nfYV8uOSr\nwB0ppYdLm7iRHCCujYjPAouALwLfTCntnPjbkSRJRap0j8UC4BpyIOgFfgUsSyndGhGLgYvIh0Xm\nksPHvwF/0//klFJfRLwP+CfylSRbge8Cn5/Y25AkSfWgomCRUvrjEZatA84bwzaeB95XyetKkqTG\n4FDaTayzs7PoEhqS/VY5+2x87LfK2Wf1b8Ijb06GiGgHuru7uz1pR5KkCvT09NDR0QHQkVLqqfXr\nucdCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFC\nkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRVjcFCkiRV\njcFCkiRVTUMFix07iq5AkiSNpKGCRXd30RVIkqSRNFSwuOuuoiuQJEkjMVhIkqSqaahgsW4dPPlk\n0VVIkqThNFSwAFi5sugKJEnScAwWkiSpahouWNx5J2zZUnQVkiRpKA0XLHbuhJtvLroKSZI0lIYL\nFuDhEEmS6lVDBYuZM/PPVasgpWJrkSRJb1dRsIiIT0TEQxHRW5rujohLypbPjIh/jIhNEbElIn4Y\nEfMHbWNJRKyMiK0RsT4iroqIMdVx8sn550svwYMPVlK5JEmaDJXusXge+CzQUZpuBf49IpaWln8d\nuAz4IHAOcBDwo/4nlwLEKmAacBrwMeDjwBfG8uJnnTXQ9nCIJEn1p6JgkVJamVJanVJ6qjR9DngD\nOC0i9gH+EFiRUrojpfQg8AfAmRFxSmkTy4GjgStSSmtTSmuAPwc+GRHTRnv98mCxalUllUuSpMkw\n7nMsImJKRHwYmAPcQ96DMQ24pX+dlNITwHPA6aVZpwFrU0qbyja1BmgDjhntNRctgmOPze377oON\nG8dbvSRJqoWKg0VEHBsRW4DtwNXAB1JKjwMLgR0ppdcHPWVDaRmlnxuGWE7ZOiN673vzz5Rg9epK\nq5ckSbU0nj0WjwMnAKcC/wRcExFHj7B+AGO5hmNM13lcdtlA2/MsJEmqL6Oe1zBYSmkX8HTpYU/p\n/IlPAz8AZkTEPoP2WsxnYK/EeuDkQZtcUPo5eE/G26xYsYJ99mlj2jTYtQt+9CO47rpOrryys9K3\nIUlS0+nq6qKrq2uPeb29vZNaQ8XBYghTgJlAN7ALuBD4MUBEHAUcAtxdWvce4D9HxLyy8yyWAb3A\no6O90Ne+9jXa29v58Ifhf/yPHC4OPbQK70CSpCbQ2dlJZ+ee/9nu6emho6Nj0mqodByLv4mIsyLi\n0NK5Fn8LnAtcV9pL8S/AVyPivIjoAL4D/Dyl9EBpEzeSA8S1EXF8RCwHvgh8M6W0c6x1eDhEkqT6\nVOk5FguAa8jnWdxMvhJkWUrp1tLyFcBPgR8CtwMvkse0ACCl1Ae8D9hN3otxDfBd4POVFHHJJRCR\n2wYLSZLqR0WHQlJKfzzK8u3Ap0rTcOs8Tw4X43bggXDKKfmS04cfhueeg0MOmcgWJUlSNTTUvULK\neThEkqT60xTBwlE4JUmqDw0bLE48MY/ECXDLLfDmm8XWI0mSGjhYRMCll+b2m2/C7bcXWo4kSaKB\ngwV4noUkSfWmoYPFxRfD9Om5vXJlvn+IJEkqTkMHi733hnPOye1nn4XHHy+0HEmSWl5DBwvwcIgk\nSfWk4YNF/23UwWAhSVLRGj5YHHUUvOMduX3XXTDJN3GTJEllGj5YRAwcDtm1C268sdh6JElqZQ0f\nLMBROCVJqhdNESzOPRfmzMntVaugr6/YeiRJalVNESxmzoSLLsrtl1+G7u5i65EkqVU1RbAALzuV\nJKkeNE2w8LJTSZKK1zTBYvFiOOGE3P7FL2DDhmLrkSSpFTVNsIA9D4fccENxdUiS1KqaKlh4OESS\npGI1VbA47TTYf//cvvFG2Lmz2HokSWo1TRUspk6FSy7J7ddfz0N8S5KkydNUwQIchVOSpCI1XbBY\nvhymlN6V51lIkjS5mi5YHHBAPtcC4LHH4Jlniq1HkqRW0nTBAhyFU5KkohgsJElS1TRlsDj+eDj4\n4Ny+7TbYtq3YeiRJahVNGSwiBgbL2r4dbr212HokSWoVTRkswMMhkiQVoWmDxYUXwowZub1yJaRU\nbD2SJLWCpg0We+0F552X288/Dw8/XGg5kiS1hKYNFuAonJIkTbamDhbe7VSSpMnV1MHiyCPhqKNy\n++67YfPmYuuRJKnZVRQsIuLPIuL+iHg9IjZExI8j4qhB69weEX1l0+6IuHrQOksiYmVEbI2I9RFx\nVUTUJOT0Hw7ZvRvWrKnFK0iSpH6V/jE/G/gGcCpwETAduDEiZpetk4D/BiwAFgKLgM/0LywFiFXA\nNOA04GPAx4EvjOsdjMLLTiVJmjzTKlk5pfTe8scR8XHgZaADuKts0baU0sZhNrMcOBo4P6W0CVgb\nEX8O/F1E/GVKaVclNY3m7LPzFSJvvAGrV+c9F1OnVvMVJElSv4keftiXvIfi1UHzr4iIjRGxNiK+\nNGiPxmnA2lKo6LcGaAOOmWA9bzNjBlx8cW5v2gQPPFDtV5AkSf3GHSwiIoCvA3ellB4tW/Q94Erg\nPOBLwO8D15YtXwhsGLS5DWXLqs7DIZIkTY6KDoUMcjXwbuDM8pkppW+XPXwkItYDt0TE4SmlZ0bZ\n5ojjY65YsYK2trY95nV2dtLZ2TniRgdfdvrFL45ShSRJDairq4uurq495vX29k5qDZHGMdZ1RHwT\neD9wdkrpuVHWnQO8ASxPKd0UEX8FvD+l1F62zmHA08CJKaWHhthGO9Dd3d1Ne3v74MVj0tEBPT25\n/cILcNBB49qMJEkNpaenh46ODoCOlFJPrV+v4kMhpVDxO+STL0cMFSUnkvdEvFR6fA9wXETMK1tn\nGdALPEqNlB8OueGGWr2KJEmtrdJxLK4GrgA+AmyNiAWlaVZp+RER8bmIaI+IQyPicuBfgTtSSv13\n67iRHCCujYjjI2I58EXgmymlndV6Y4M5CqckSbVX6R6LTwD7ALcDL5ZNHyot30Ee32IN8BjwFeDf\ngMv7N5BS6gPeB+wG7gauAb4LfH58b2FsTj4Z5pX2kdx0E2zfXstXkySpNVU6jsWIQSSltI58Ncho\n23meHC4mzdSpcOmlcO21eUyLn/0MLrpoMiuQJKn5NfW9QgbzslNJkmqrpYLFsmUDo256G3VJkqqv\npYLFfvvBGWfk9pNPwlNPFVuPJEnNpqWCBXg4RJKkWjJYSJKkqmm5YHHMMXDIIbl9xx35ChFJklQd\nLRcsIgb2WuzYAbfcUmw9kiQ1k5YLFuAonJIk1UpLBosLLoBZs3J71SoYx33YJEnSEFoyWMyZA+ef\nn9svvAAPve1+qpIkaTxaMliAV4dIklQLLRssys+zcBROSZKqo2WDxeGHw9KluX3vvfDKK8XWI0lS\nM2jZYAEDh0P6+mD16mJrkSSpGRgsSjzPQpKkiWvpYHHmmdDWlturV8OuXcXWI0lSo2vpYDF9er6V\nOsDmzXDffcXWI0lSo2vpYAGOwilJUjW1fLC49NKBtsFCkqSJaflgsWABnHxybv/qV/D888XWI0lS\nI2v5YAF7Xh3iYFmSJI2fwQKDhSRJ1WKwANrb8yERgJtvhrfeKrYeSZIalcECmDJl4CTObdvgjjuK\nrUeSpEZlsChxFE5JkibOYFFy8cUwbVpur1wJKRVbjyRJjchgUdLWBmefndtPPw1PPllsPZIkNSKD\nRRlH4ZQkaWIMFmU8z0KSpIkxWJQ5+mg4/PDcvvNOeP31YuuRJKnRGCzKRAzstdi1C266qdh6JElq\nNAaLQRyFU5Kk8TNYDHLuuTB7dm6vWgV9fcXWI0lSI6koWETEn0XE/RHxekRsiIgfR8RRg9aZGRH/\nGBGbImJLRPwwIuYPWmdJRKyMiK0RsT4iroqIugg5s2fDhRfm9vr18OCDxdYjSVIjqfSP+dnAN4BT\ngYuA6cCNETG7bJ2vA5cBHwTOAQ4CftS/sBQgVgHTgNOAjwEfB74wrndQA14dIknS+FQULFJK700p\nXZtSeiyltJYcCA4BOgAiYh/gD4EVKaU7UkoPAn8AnBkRp5Q2sxw4GrgipbQ2pbQG+HPgkxExrSrv\naoIcz0KSpPGZ6OGHfYEEvFp63EHeE3FL/woppSeA54DTS7NOA9amlDaVbWcN0AYcM8F6quKQQ+C4\n43L7gQfg5ZeLrUeSpEYx7mAREUE+7HFXSunR0uyFwI6U0uARIDaUlvWvs2GI5ZStU7j+vRYpwerV\nxdYiSVKjmMihh6uBdwNnjWHdIO/ZGM2I66xYsYK2trY95nV2dtLZ2TmGTVfmssvgy1/O7ZUr4aMf\nrfpLSJJUVV1dXXR1de0xr7e3d1JriDSO23hGxDeB9wNnp5SeK5t/PnAzsF/5XouIeBb4WkrpHyLi\nr4D3p5Tay5YfBjwNnJhSemiI12sHuru7u2lvbx+8uCZ27YL582Hz5nyDso0bYfr0SXlpSZKqpqen\nh46ODoCOlFJPrV+v4kMhpVDxO8D55aGipBvYBVxYtv5R5BM87y7Nugc4LiLmlT1vGdALPEqdmDYN\nli/P7d5euPvukdeXJEmVj2NxNXAF8BFga0QsKE2zAEp7Kf4F+GpEnBcRHcB3gJ+nlB4obeZGcoC4\nNiKOj4jlwBeBb6aUdlbnbVWHo3BKklSZSvdYfALYB7gdeLFs+lDZOiuAnwI/LFvvg/0LU0p9wPuA\n3eS9GNcA3wU+X3n5tbV8eb5/CHjZqSRJY1HRyZsppVGDSEppO/Cp0jTcOs+Tw0VdO/BAOPVUuPde\neOQR+O1v4dBDi65KkqT6VRfDaNczR+GUJGnsDBajMFhIkjR2BotRvOc9sGhRbt96K7z5ZrH1SJJU\nzwwWo4gYGIXzrbfgttuKrUeSpHpmsBgDD4dIkjQ2BosxuOiigVE3V67M9w+RJElvZ7AYg733hnPP\nze3f/hYerZvxQSVJqi8GizFyFE5JkkZnsBij/hM4wfMsJEkajsFijI46Co48Mrfvugtee63YeiRJ\nqkcGiwr0Hw7ZvRtuvLHYWiRJqkcGiwp42akkSSMzWFTgnHNg7tzcvuEG6Osrth5JkuqNwaICM2fm\nMS0ANm6EX/yi2HokSao3BosKeThEkqThGSwq5GWnkiQNz2BRoYMPznc8BejuhpdeKrYeSZLqicFi\nHMoPh6xeXVwdkiTVG4PFOHg4RJKkoRksxuHUU+GAA3L7xhthx45i65EkqV4YLMZh6lS45JLc3rIl\nD/EtSZIMFuPmZaeSJL2dwWKcli+HKaXe8zbqkiRlBotx2n9/OP303H78cXj66WLrkSSpHhgsJsDD\nIZIk7clgMQEGC0mS9mSwmIDjjoPFi3P79tth69ZCy5EkqXAGiwmIGBgsa/t2uPXWYuuRJKloBosJ\n8nCIJEkDDBYTdOGFMHNmbq9cCSkVW48kSUUyWEzQ3Llw3nm5vW4drF1baDmSJBXKYFEFHg6RJCkz\nWFRB+d1OHYVTktTKKg4WEXF2RFwfES9ERF9EXD5o+XdK88unVYPW2S8ivhcRvRGxOSK+HRFzJ/pm\nivKOd8C73pXbd98Nr75abD2SJBVlPHss5gK/BD4JDHeq4g3AAmBhaeoctPz7wFLgQuAy4BzgW+Oo\npW70Hw7p64M1a4qtRZKkolQcLFJKq1NKf5FS+gkQw6y2PaW0MaX0cmnq7V8QEUcDy4E/Sin9IqV0\nN/Ap4MMRsXA8b6IeeJ6FJEm1O8fivIjYEBGPR8TVEbF/2bLTgc0ppQfL5t1M3vtxao3qqbmzzoK9\n987t1ath9+5i65EkqQi1CBY3AB8FLgA+A5wLrIqI/r0bC4GXy5+QUtoNvFpa1pBmzICLL87tV16B\n++8vth5JkopQ9WCRUvpBSumnKaVHUkrXA+8DTgHOG+WpwfDnbDQED4dIklrdtFq/QErpmYjYBBwJ\n3AasB+aXrxMRU4H9gA0jbWvFihW0tbXtMa+zs5POzsHnhhaj/LLTlSvhr/+6uFokSa2nq6uLrq6u\nPeb19vYOs3ZtRJrAGNQR0Qf8bmnPxHDrLAZ+C/xOSumnpZM3HwFO6j/PIiKWAauAxSml9UNsox3o\n7u7upr29fdz1ToaTToLu7txetw4OPrjYeiRJra2np4eOjg6AjpRST61fbzzjWMyNiBMi4j2lWUeU\nHi8pLbsqIk6NiEMj4kLgJ8CTwBqAlNLjpfY/R8TJEXEm8A2ga6hQ0WjKD4c4WJYkqdWM5xyLk4AH\ngW7yORF/D/QAfwXsBo4H/h14Avhn4AHgnJTSzrJtfAR4nHw1yE+BO4E/Gd9bqC+OwilJamUVn2OR\nUrqDkQPJJWPYxmvAlZW+diM4+WQ48EDYuBFuugm2bx+4+6kkSc3Oe4VU2ZQpcOmlub11K9x5Z7H1\nSJI0mQwWNeBlp5KkVmWwqIFly2Dq1Nw2WEiSWonBogb23RfOPDO3n3oKfv3rYuuRJGmyGCxqxMMh\nkqRWZLCoEYOFJKkVGSxq5N3vhkMPze077oAtW4qtR5KkyWCwqJGIgb0WO3fCLbcUW48kSZPBYFFD\ng29KJklSszNY1ND558OsWbm9ahVM4H5vkiQ1BINFDc2ZAxdckNsvvgi//GWx9UiSVGsGixrz6hBJ\nUisxWNSYdzuVJLUSg0WNHXZYvvQU4N57YdOmQsuRJKmmDBaToP9wSEqwenWxtUiSVEsGi0ngeRaS\npFZhsJgEZ5wBbW25vXo17NpVbD2SJNWKwWISTJ8Oy5fn9muv5XMtJElqRgaLSeIonJKkVmCwmCSX\nXprvHwIGC0lS8zJYTJL58+Hkk3N77Vp47rli65EkqRYMFpOo/OoQB8uSJDUjg8UkMlhIkpqdwWIS\nnXgiLFiQ27fcAm+9VWw9kiRVm8FiEk2ZMnB1yLZtcPvthZYjSVLVGSwmmaNwSpKamcFikl18cR4w\nC3KwSKnYeiRJqiaDxSTbZx84++zcfuYZeOKJYuuRJKmaDBYFcBROSVKzMlgUwPMsJEnNymBRgHe9\nC444Ird/9jPo7S22HkmSqsVgUYCIgb0Wu3bBTTcVW48kSdVisCiIo3BKkppRxcEiIs6OiOsj4oWI\n6IuIy4dY5wsR8WJEbIuImyLiyEHL94uI70VEb0RsjohvR8TcibyRRnPuuTBnTm6vWgV9fcXWI0lS\nNYxnj8Vc4JfAJ4G3jcIQEZ8F/jfgT4BTgK3AmoiYUbba94GlwIXAZcA5wLfGUUvDmjULLrwwtzds\ngJ6eYuuRJKkaKg4WKaXVKaW/SCn9BIghVvk08MWU0v+XUnoY+ChwEPC7ABGxFFgO/FFK6RcppbuB\nTwEfjoiF430jjcirQyRJzaaq51hExOHAQuCW/nkppdeB+4DTS7NOAzanlB4se+rN5L0fp1aznnrn\neBaSpGZT7ZM3F5IDwoZB8zeUlvWv83L5wpTSbuDVsnVawpIlcPzxuf3AA/mQiCRJjWyyrgoJhjgf\nYxzrNJ3yvRarVxdXhyRJ1TCtyttbTw4IC9hzr8V84MGydeaXPykipgL78fY9HXtYsWIFbW1te8zr\n7Oyks7NzYlUX6LLL4O/+LrdXroSPfazYeiRJjaurq4uurq495vVO8iiMkSZwe82I6AN+N6V0fdm8\nF4GvpJS+Vnq8DzkwfDSl9G8RcTTwCHBS/3kWEbEMWAUsTimtH+J12oHu7u5u2tvbx11vPdq1C+bP\nh82b8w3KNm0auPupJEkT1dPTQ0dHB0BHSqnm1yCOZxyLuRFxQkS8pzTriNLjJaXHXwc+FxHvj4jj\ngGuAdcC/A6SUHgfWAP8cESdHxJnAN4CuoUJFs5s2DS65JLdffx1+/vNi65EkaSLGc47FSeTDGt3k\ncyL+HugB/gogpXQVOSh8i3w1yGzg0pTSjrJtfAR4nHw1yE+BO8njXrQkR+GUJDWLis+xSCndwSiB\nJKX0l8BfjrD8NeDKSl+7WS1fnu8fklI+z+Kqq4quSJKk8fFeIXVg3jw47bTcfvRRePbZQsuRJGnc\nDBZ1wlE4JUnNwGBRJwwWkqRmYLCoEyecAAcdlNu33QbbthVbjyRJ42GwqBMRA6NwvvVWDheSJDUa\ng0Ud8XCIJKnRGSzqyEUXwYwZub1yZb78VJKkRmKwqCN77QXnnpvbzz0HjzxSbD2SJFXKYFFnHIVT\nktTIDBZ1pvw26p5nIUlqNAaLOvPOd+YJ8g3JNm8uth5JkiphsKhD/YdDdu+GG28sthZJkiphsKhD\nXnYqSWpUBos6dPbZ+QoRgBtugL6+YuuRJGmsDBZ1aObMPKYFwKZN8MADxdYjSdJYGSzqlIdDJEmN\nyGBRp7zsVJLUiAwWdeqgg+DEE3O7pwdeeqnYeiRJGguDRR0rPxxyww3F1SFJ0lgZLOqYh0MkSY3G\nYFHHTjkF5s3L7Ztugh07iq1HkqTRGCzq2NSpcMklub1lC/zsZ8XWI0nSaAwWdc7LTiVJjcRgUeeW\nLYMppX8lb6MuSap3Bos6t//+cMYZuf3EE/Cb3xRbjyRJIzFYNAAPh0iSGoXBogEYLCRJjcJg0QCO\nPRaWLMnt22+HN94otBxJkoZlsGgAEQODZe3YAbfeWmw9kiQNx2DRIDwcIklqBAaLBnHBBTBzZm6v\nWgUpFVuPJElDMVg0iLlz4fzzc3vdOvjVr4qtR5KkoRgsGoiHQyRJ9c5g0UDK73bqKJySpHpU9WAR\nEZ+PiL5B06Nly2dGxD9GxKaI2BIRP4yI+dWuoxkdcQQcfXRu33MPvPJKsfVIkjRYrfZYPAwsABaW\nprPKln0duAz4IHAOcBDwoxrV0XT6D4f09cGaNcXWIknSYLUKFrtSShtTSi+XplcBImIf4A+BFSml\nO1JKDwJ/AJwZEafUqJam4nkWkqR6Vqtg8c6IeCEifhMR10VEadxIOoBpwC39K6aUngCeA06vUS1N\n5ayzYO+9c3v1ati9u9h6JEkqV4tgcS/wcWA58AngcODOiJhLPiyyI6X0+qDnbCgt0yimT8+3Ugd4\n9VW4775i65Ekqdy0am8wpVR+5P/hiLgf+C3wIeCtYZ4WwKhDPq1YsYK2trY95nV2dtLZ2TnOahvT\nZZfBj0pnpaxcOXBbdUlSa+vq6qKrq2uPeb29vZNaQ6RJGMKxFC5uAm4uTfuV77WIiGeBr6WU/mGY\n57cD3d3d3bS3t9e83nq3fj0sWpTbJ5wAv/xlsfVIkupXT08PHR0dAB0ppZ5av17Nx7GIiL2AdwAv\nAt3ALuDCsuVHAYcA99S6lmaxcCGcdFJuP/RQHolTkqR6UItxLL4SEedExKERcQbwY3KY+O+lvRT/\nAnw1Is6LiA7gO8DPU0r3V7uWZlZ+dYiDZUmS6kUt9lgsBr4PPA78d2AjcFpKqX84pxXAT4EfAreT\n92R8sAZ1NDVH4ZQk1aNanLw54pmUKaXtwKdKk8bppJNg/nx4+WW4+WbYvn3g7qeSJBXFe4U0qClT\n4NJLc3vrVrjjjmLrkSQJDBYNzVE4JUn1xmDRwJYtg6lTc3vlSpiEK4clSRqRwaKBtbXlIb4BfvMb\n+PWvi61HkiSDRYPzcIgkqZ4YLBqcwUKSVE8MFg1u6VI47LDcvvNO2LKl0HIkSS3OYNHgIgb2Wuzc\nCTfdVGw9kqTWZrBoAo7CKUmqFwaLJnD++TB7dm6vWuVlp5Kk4hgsmsDs2XDBBbn90kvw4IPF1iNJ\nal0Giybh1SGSpHpgsGgS5edZGCwkSUUxWDSJQw+FY47J7fvvh40bi61HktSaDBZNpP9wSEqwenWx\ntUiSWpPBool4noUkqWgGiyZyxhmw7765vWYN7NpVbD2SpNZjsGgi06bB8uW5/dprcPfdxdYjSWo9\nBosm4yickqQiGSyazKWX5vuHgOdZSJImn8GiyRx4IJxySm4//DA891yx9UiSWsu0ogtQ9V12Gdx3\nX25ffTW8730wYwbMnDnyzynGTEnSBBksmtBll8Ff/EVuf/nLeRqLadPGFkBmzhzbOtX8Oc1PqiQ1\nBH9dN6E8o2HyAAAODElEQVT3vAfe+U749a8re96uXXnatq02dU3ElCmTF2T22w8WL4YlS2DOnKLf\nuSQ1FoNFE5oyJY+8+YMfwJYtsGMHbN/+9p9DzRvp586dxb2nvj546608Tab9988Boz9oDNWeNWty\na5KkemawaFJHHAH/6T9Vd5t9fTlcVBpIJuPn9u3Vfa/9Xn01Tw89NPw68+YNHzqWLIGDD857QiSp\nFRgsNGZTpgycX1FvUsqHccYSQEZbZ+NGeP55WLcu/3zhhZH31mzalKcHHxx+nQULht/rsWQJHHQQ\nTJ9e/X6RmsHWrQPfx1degQMOgEWL8rTffgOX2Ks+GCzUFCLyH+bp02Hu3Opuu68PNmwY+MVWHjr6\npxdfhN27h9/Ghg156u4evv6FC0c+7LJokSexqvls2TLwnSqfyue99trwz58xI393Fi0a+Fne7v+5\nYIHhfbL4a0oaxZQpA7+sTj556HV274b164cOHf2PX3oph5ShpJSXv/RSvu39SHUMt9dj8eL8S3Tq\n1Oq8b2kiUoLe3pEDw7p18PrrE3udHTvyeD1jGbNn3ry3B4+h2nvvPbGaWp3BQqqCqVPzuRQHHzz8\nOrt25eAwVOjob69fn38hD6WvLx+WeeEFuPfeodeZNi0fVhnpsMv8+Y5ZoolJCTZvHjkwrFsHb7wx\nsdeZMSN/fvs/w4sX58Mgr7ySv0vr1w8E8k2bRt9e/2HLtWtHXm/u3NHDx6JFOaj4XXo7g4U0SaZN\nG/jjPpwdO/JhlaFCR3/75ZeHf/6uXaP/72369ByAhtvrsWRJHsHV49atKaX8h3ukwLBu3cQvS581\na+AzVz6Vz5s3b+yfw5078+HG8sBRHjzK5+3YMfK2tm6Fp57K00imTs2HWEYLIQsXttbVYwYLqY7M\nmAGHHZan4WzfnvdaDLfX4/nnR/7f286d8OyzeRqpjuFCR/n/HA0fjaWvL382hgsM/fMmepXVnDlv\nDw2DH++/f3U/P9OnD2x7JP17WwYHjqHavb0jb2v37vwfgRdfHL2+/fYbfQ/IokXQ1tb43yuDRRPr\n6uqis7Oz6DIaTr3328yZ+XLiI44Yfp0339zzj8VQ535s3jz883fsgKefztNwZs8e+EW+Y0cXxx3X\nyezZ+Y/K7NkDU/njkZbNnt1654dU87PW15f3Zo0UGl54YfT/rY9mr732DJqD9zgsXgz77lu7P44T\n7bOIHGr23x+OOWbkdbdtywFjuD0f/fNefnn486f6bd6cp8ceG3m9WbPefuLpUCFk/vz6PZk70nAH\ndCfjxSM+CfwpsBB4CPhUSumBIdZrB7q7u7tpb2+f5Cob1+WXX871119fdBkNp1X6rfwSvuFOOB3t\nf2wDLgcm3mczZow9hEwkwMyZkwNa0f8zHOtnbffuvJt/pL0ML7yQD4VNRFvb8IGhf94++0zsNSaq\nHr+fu3fny9RH2wPy0ks59FdDRD5kOZaTUZ94ooeOjg6AjpRST3UqGF5heScifg/4e+B/Be4HVgBr\nIuKolNIYTsORNBFz58K73pWn4WzZMnzo6J8meoJeuR078jT2QDN+Efl/h9UOLMM9Hu5Sx/6Teke6\nemK0y5nHYr/9Rg4MBx/s1RDjNXXqwLkUI0kpXwUzWvhYvz6f5zLatl5+OU8jDeAHk39rgiJ3pKwA\nvpVSugYgIj4BXAb8IXBVgXVJKtl7b1i6NE/D6e2FD3wA/st/yf8b65+2bRu6PZ7Ho+1mHo+UBrY/\nGaZN2zN0zJ6dz3OZOXPi72/evJFPgjz44OqP76LKReS9Qm1tIwd6yOe5bNgwthAy2p6qyb7/UyHB\nIiKmAx3Al/rnpZRSRNwMnF5ETZLGp60tH3ev1VHKlPJejLEGkYmGmFrdj2bXrrwHaMuWyp43f/7w\nJ0D2h4bZs2tTs4ozcyYcckieRtLXl287MFL4eOaZvAdsshS1x2IeMBXYMGj+BmCoHDcL4LHRznrR\nHnp7e+npqfnhtKZjv1WuiD6bOjUHmr32qu52+/oGhn/fvj0HjfL2aI+H+tk/vfnmno937OjlyCN7\nWLgwB4gFCwZ+LliQj6HPmDF8ra+/PvEBphqN38/hzZuXp2OP3XP+Y489xpVXAqW/pbVWyMmbEbEI\neAE4PaV0X9n8q4CzUkpnDFr/I8D3JrdKSZKayhUppe/X+kWK2mOxCdgNLBg0fz5v34sBsAa4AngW\nmOQbZ0uS1NBmAYeR/5bWXGGXm0bEvcB9KaVPlx4H8BzwX1NKXymkKEmSNCFFXhXyVeBfI6KbgctN\n5wDfLbAmSZI0AYUFi5TSDyJiHvAF8iGRXwLLU0obi6pJkiRNTKEjb0qSpObiDV8lSVLVGCwkSVLV\n1H2wiIhPRsQzEfFmRNwbEScXXVNRIuLzEdE3aHq0bPnMiPjHiNgUEVsi4ocRMX/QNpZExMqI2BoR\n6yPiqoio+89BJSLi7Ii4PiJeKPXR5UOs84WIeDEitkXETRFx5KDl+0XE9yKiNyI2R8S3I2LuoHWO\nj4g7S5/N30bE/1nr91Yro/VZRHxniM/eqkHrtFqf/VlE3B8Rr0fEhoj4cUQcNWidqnwnI+K8iOiO\niLci4smI+NhkvMdaGGO/3T7os7Y7Iq4etE7L9FtEfCIiHip9t3oj4u6IuKRseX19zlJKdTsBv0ce\nt+KjwNHAt4BXgXlF11ZQf3we+BVwIHnMj/nA/mXL/4k81se5wInA3cDPypZPAdaSr2U+DlgOvAz8\nddHvrcr9dAn5pODfJY+Xcvmg5Z8tfY7eDxwL/AT4DTCjbJ0bgB7gJOAM4EngurLlewMvAf8KLAU+\nBGwF/rjo91+jPvsOsHLQZ69t0Dqt1mergN8vvZfjgJ+Wvn+zy9aZ8HeSPP7AG+R7KL0L+CSwE7i4\n6D6oYb/dBvzfgz5ve7Vqv5Hvo3UJcGRp+mtgO7C0Hj9nhXfYKJ15L/APZY8DWAd8pujaCuqPzwM9\nwyzbp/RB+0DZvHcBfcAppceXlj4o88rW+RNgMzCt6PdXoz7r4+1/JF8EVgzquzeBD5UeLy0978Sy\ndZYDu4CFpcf/gTzQ27Sydf4WeLTo91yjPvsO8P+O8JyjW7nPSu9lXqkPzir7XE34Owl8GfjVoNfq\nAlYV/Z5r0W+lebcBXx3hOfYbvAL8QT1+zup2F3gM3Kjslv55Kb/TVr9R2TtLu6t/ExHXRcSS0vwO\n8uXD5f31BHnQsf7+Og1Ym/a8Lf0aoA04pvalFy8iDgcWsmc/vQ7cx579tDml9GDZU28GEnBq2Tp3\nppTK7yu4BnhXRLTVqPyinVfadf14RFwdEfuXLTsd+2xf8vt9tfS4Wt/J08h9yaB1muX34OB+63dF\nRGyMiLUR8aWIKL/VWsv2W0RMiYgPk8d9uoc6/JzVbbBg5BuVjXLX+6Z1L/Bx8v8EPwEcDtxZOo69\nENhR+iNZrry/FjJ0f0Lr9OlC8i+xkT5XC8m7Cf+nlNJu8i++Vu3LG8iHJC8APkPe5boqIqK0vKX7\nrNQPXwfuSin1n/dUre/kcOvsExEzJ1p7kYbpN8j3hroSOI98F+zfB64tW95y/RYRx0bEFvLeiavJ\neygepw4/Z0WOvDleQf7D0HJSSuXjvD8cEfcDvyUfqx7uHipj7a+W7NMyY+mn0dbp/yPbdH2ZUvpB\n2cNHImIt+byU88i7rYfTKn12NfBu4KwxrFuN72Sz9duZ5TNTSt8ue/hIRKwHbomIw1NKz4yyzWbt\nt8eBE8h7eD4IXBMR54ywfmGfs3reY1HpjcpaTkqpl3yC3JHAemBGROwzaLXy/lrP2/uz/3Gr9Ol6\n8pdlpM/V+tLj/ykipgL7lZb1rzPUNqAF+rL0y30T+bMHLdxnEfFN4L3AeSmlF8sWTfQ7OVq/vZ5S\n2jGR2os0qN9eGmX1/rtgl3/eWqrfUkq7UkpPp5R6Ukr/F/AQ8Gnq8HNWt8EipbQT6AYu7J9X2m12\nIfmM15YXEXsB7yCfjNhNPlGuvL+OAg5hoL/uAY6LPJR6v2VAL1C+G7Jplf4grmfPftqHfB5AeT/t\nGxEnlj31QnIgub9snXNKfzz7LQOeKAW+phYRi4EDyFd5QIv2WemP4+8A56eUnhu0eKLfycfK1rmQ\nPS0rzW9Io/TbUE4k/6+5/PPWcv02yBRgJvX4OSv6zNZRznr9EPls/fLLTV8BDiy6toL64yvAOcCh\n5Mv5biIn0gNKy68GniHvnu4Afs7bLzl6iHy8/HjyuRobgC8W/d6q3E9zybsM30M+M/p/Lz1eUlr+\nmdLn6P3kS69+AvyaPS83XQX8AjiZvJv2CeDasuX7kAPdv5J35f4e+VKtPyr6/Ve7z0rLriKHr0PJ\nv3x+Qf6FNL2F++xq8ln1Z5P/p9c/zRq0zoS+kwxcBvhl8tn+/xHYAVxUdB/Uot+AI4DPAe2lz9vl\nwFPAra3ab8DfkA+zHUq+RP5vyWHignr8nBXeYWPo0P9Ivj73TXJyOqnomgrsiy7y5bZvks/4/T5w\neNnymcA3yLuotwD/BswftI0l5OvG3yh9sL4MTCn6vVW5n84l/3HcPWj6f8rW+UvyH7lt5DOfjxy0\njX2B68iJfjPwz8CcQescB9xR2sZzwJ8W/d5r0WfALGA1eU/PW8DT5OvmDxy0jVbrs6H6azfw0bJ1\nqvKdLP37dJe++78Gfr/o91+rfgMWA7cDG0ufkyfIf0j3GrSdluk34Nul792bpe/hjZRCRT1+zrwJ\nmSRJqpq6PcdCkiQ1HoOFJEmqGoOFJEmqGoOFJEmqGoOFJEmqGoOFJEmqGoOFJEmqGoOFJEmqGoOF\nJEmqGoOFJEmqGoOFJEmqmv8fo0vni0F2r6wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe768159b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuronal Network Model With L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "l2 = 0.0001\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases,l2)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 314.981872559\n",
      "Minibatch accuracy: 7.8125\n",
      "Validation accuracy: 27.28\n",
      "Minibatch loss at step 500: 10.8104972839\n",
      "Minibatch accuracy: 82.8125\n",
      "Validation accuracy: 80.02\n",
      "Minibatch loss at step 1000: 6.50378417969\n",
      "Minibatch accuracy: 83.59375\n",
      "Validation accuracy: 80.5\n",
      "Minibatch loss at step 1500: 4.54020404816\n",
      "Minibatch accuracy: 82.8125\n",
      "Validation accuracy: 81.54\n",
      "Minibatch loss at step 2000: 2.44680547714\n",
      "Minibatch accuracy: 85.9375\n",
      "Validation accuracy: 81.2\n",
      "Minibatch loss at step 2500: 8.33030700684\n",
      "Minibatch accuracy: 84.375\n",
      "Validation accuracy: 81.44\n",
      "Minibatch loss at step 3000: 2.44528198242\n",
      "Minibatch accuracy: 86.71875\n",
      "Validation accuracy: 82.88\n",
      "Test accuracy: 89.38\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZJUNwYLSZJUNwYLSZJUNwYLSZJUN/8f/6HEHsz+/D4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe76a61c750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

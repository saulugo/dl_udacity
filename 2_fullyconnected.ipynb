{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning - Prueba Saul\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 28, 28) (100000,)\n",
      "Validation set (5000, 28, 28) (5000,)\n",
      "Test set (5000, 28, 28) (5000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 784) (100000, 10)\n",
      "Validation set (5000, 784) (5000, 10)\n",
      "Test set (5000, 784) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 18.421494\n",
      "Training accuracy: 12.8%\n",
      "Validation accuracy: 12.8%\n",
      "Loss at step 100: 0.437522\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 11.5%\n",
      "Loss at step 200: 0.243619\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 11.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7d13d750519f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# and get the loss value and the training predictions returned as numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss at step %d: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 14.429139\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 15.9%\n",
      "Minibatch loss at step 500: 1.240521\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 21.8%\n",
      "Minibatch loss at step 1000: 0.071556\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 27.1%\n",
      "Minibatch loss at step 1500: 0.424953\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 28.8%\n",
      "Minibatch loss at step 2000: 0.572591\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 23.6%\n",
      "Minibatch loss at step 2500: 0.620691\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 25.6%\n",
      "Minibatch loss at step 3000: 0.209356\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 28.7%\n",
      "Test accuracy: 31.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First lets define a fucntion to represent the topology of our Neuronal Network:\n",
    "#Topology: Multilayer Perceptron, 1 hidden layer with 1024 neurons and RELU activation function.\n",
    "\n",
    "def mlp(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    #h1 layer z = XW + b\n",
    "    h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "    #h1 layer activation function relu(z)\n",
    "    h1_layer = tf.nn.relu(h1_layer)\n",
    "    #output layer (no activation needed after output layer)\n",
    "    out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 0\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "\n",
    "Graph Variables Initialized!\n",
    "Minibatch loss at step 0: 263.086303711\n",
    "Minibatch accuracy: 15.625\n",
    "Validation accuracy: 10.0\n",
    "Minibatch loss at step 500: 0.0\n",
    "Minibatch accuracy: 100.0\n",
    "Validation accuracy: 19.38\n",
    "Minibatch loss at step 1000: 0.0\n",
    "Minibatch accuracy: 100.0\n",
    "Validation accuracy: 19.94\n",
    "Minibatch loss at step 1500: 0.115932941437\n",
    "Minibatch accuracy: 98.4375\n",
    "Validation accuracy: 21.96\n",
    "Minibatch loss at step 2000: 0.0125381769612\n",
    "Minibatch accuracy: 99.21875\n",
    "Validation accuracy: 27.06\n",
    "Minibatch loss at step 2500: 8.1004447937\n",
    "Minibatch accuracy: 91.40625\n",
    "Validation accuracy: 32.54\n",
    "Minibatch loss at step 3000: 0.155385047197\n",
    "Minibatch accuracy: 97.65625\n",
    "Validation accuracy: 29.34\n",
    "Test accuracy: 33.46\n",
    "\n",
    "\n",
    "\n",
    "# Test 1\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.001\n",
    "display_step = 100\n",
    "n_hidden_1 = 1024\n",
    "\n",
    "Graph Variables Initialized!\n",
    "Minibatch loss at step 0: 378.677947998\n",
    "Minibatch accuracy: 2.34375\n",
    "Validation accuracy: 9.3\n",
    "Minibatch loss at step 500: 30.2137889862\n",
    "Minibatch accuracy: 84.375\n",
    "Validation accuracy: 18.84\n",
    "Minibatch loss at step 1000: 3.44970607758\n",
    "Minibatch accuracy: 97.65625\n",
    "Validation accuracy: 25.44\n",
    "Minibatch loss at step 1500: 19.5800895691\n",
    "Minibatch accuracy: 79.6875\n",
    "Validation accuracy: 37.88\n",
    "Minibatch loss at step 2000: 13.3853445053\n",
    "Minibatch accuracy: 94.53125\n",
    "Validation accuracy: 37.42\n",
    "Minibatch loss at step 2500: 84.9151153564\n",
    "Minibatch accuracy: 50.0\n",
    "Validation accuracy: 33.3\n",
    "Minibatch loss at step 3000: 31.9156742096\n",
    "Minibatch accuracy: 80.46875\n",
    "Validation accuracy: 43.2\n",
    "Test accuracy: 48.94\n",
    "\n",
    "# Test 2\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 4001\n",
    "learning_rate = 0.001\n",
    "display_step = 100\n",
    "n_hidden_1 = 1024\n",
    "\n",
    "Graph Variables Initialized!\n",
    "Minibatch loss at step 0: 188.482727051\n",
    "Minibatch accuracy: 17.1875\n",
    "Validation accuracy: 8.42\n",
    "Minibatch loss at step 100: 41.8693504333\n",
    "Minibatch accuracy: 64.84375\n",
    "Validation accuracy: 13.42\n",
    "Minibatch loss at step 200: 11.2613897324\n",
    "Minibatch accuracy: 90.625\n",
    "Validation accuracy: 17.66\n",
    "Minibatch loss at step 300: 7.90046977997\n",
    "Minibatch accuracy: 94.53125\n",
    "Validation accuracy: 14.28\n",
    "Minibatch loss at step 400: 67.6308288574\n",
    "Minibatch accuracy: 57.03125\n",
    "Validation accuracy: 20.84\n",
    "Minibatch loss at step 500: 18.6949558258\n",
    "Minibatch accuracy: 85.15625\n",
    "Validation accuracy: 18.54\n",
    "Minibatch loss at step 600: 8.83920574188\n",
    "Minibatch accuracy: 91.40625\n",
    "Validation accuracy: 15.28\n",
    "Minibatch loss at step 700: 12.4879341125\n",
    "Minibatch accuracy: 92.1875\n",
    "Validation accuracy: 17.48\n",
    "Minibatch loss at step 800: 28.447101593\n",
    "Minibatch accuracy: 78.125\n",
    "Validation accuracy: 29.96\n",
    "Minibatch loss at step 900: 14.2780590057\n",
    "Minibatch accuracy: 88.28125\n",
    "Validation accuracy: 21.8\n",
    "Minibatch loss at step 1000: 5.75940704346\n",
    "Minibatch accuracy: 94.53125\n",
    "Validation accuracy: 24.04\n",
    "Minibatch loss at step 1100: 102.806213379\n",
    "Minibatch accuracy: 42.1875\n",
    "Validation accuracy: 29.92\n",
    "Minibatch loss at step 1200: 20.3330383301\n",
    "Minibatch accuracy: 89.84375\n",
    "Validation accuracy: 31.3\n",
    "Minibatch loss at step 1300: 13.671705246\n",
    "Minibatch accuracy: 87.5\n",
    "Validation accuracy: 26.24\n",
    "Minibatch loss at step 1400: 5.26999282837\n",
    "Minibatch accuracy: 96.875\n",
    "Validation accuracy: 21.92\n",
    "Minibatch loss at step 1500: 30.5073833466\n",
    "Minibatch accuracy: 80.46875\n",
    "Validation accuracy: 37.36\n",
    "Minibatch loss at step 1600: 12.0924053192\n",
    "Minibatch accuracy: 92.1875\n",
    "Validation accuracy: 38.42\n",
    "Minibatch loss at step 1700: 11.088558197\n",
    "Minibatch accuracy: 90.625\n",
    "Validation accuracy: 26.74\n",
    "Minibatch loss at step 1800: 80.5849304199\n",
    "Minibatch accuracy: 53.90625\n",
    "Validation accuracy: 37.06\n",
    "Minibatch loss at step 1900: 20.0569267273\n",
    "Minibatch accuracy: 87.5\n",
    "Validation accuracy: 34.2\n",
    "Minibatch loss at step 2000: 14.1156816483\n",
    "Minibatch accuracy: 92.1875\n",
    "Validation accuracy: 37.14\n",
    "Minibatch loss at step 2100: 7.57651805878\n",
    "Minibatch accuracy: 94.53125\n",
    "Validation accuracy: 31.8\n",
    "Minibatch loss at step 2200: 42.8488693237\n",
    "Minibatch accuracy: 73.4375\n",
    "Validation accuracy: 39.2\n",
    "Minibatch loss at step 2300: 8.9672498703\n",
    "Minibatch accuracy: 93.75\n",
    "Validation accuracy: 43.24\n",
    "Minibatch loss at step 2400: 6.91200876236\n",
    "Minibatch accuracy: 95.3125\n",
    "Validation accuracy: 43.14\n",
    "Minibatch loss at step 2500: 100.945175171\n",
    "Minibatch accuracy: 46.875\n",
    "Validation accuracy: 35.24\n",
    "Minibatch loss at step 2600: 30.9725379944\n",
    "Minibatch accuracy: 78.90625\n",
    "Validation accuracy: 46.12\n",
    "Minibatch loss at step 2700: 8.85964012146\n",
    "Minibatch accuracy: 90.625\n",
    "Validation accuracy: 34.54\n",
    "Minibatch loss at step 2800: 6.50729560852\n",
    "Minibatch accuracy: 96.875\n",
    "Validation accuracy: 40.76\n",
    "Minibatch loss at step 2900: 42.0945625305\n",
    "Minibatch accuracy: 70.3125\n",
    "Validation accuracy: 42.74\n",
    "Minibatch loss at step 3000: 23.724521637\n",
    "Minibatch accuracy: 82.03125\n",
    "Validation accuracy: 43.52\n",
    "Minibatch loss at step 3100: 1.39988613129\n",
    "Minibatch accuracy: 96.09375\n",
    "Validation accuracy: 46.58\n",
    "Minibatch loss at step 3200: 134.838226318\n",
    "Minibatch accuracy: 30.46875\n",
    "Validation accuracy: 47.14\n",
    "Minibatch loss at step 3300: 21.0542926788\n",
    "Minibatch accuracy: 85.9375\n",
    "Validation accuracy: 43.92\n",
    "Minibatch loss at step 3400: 12.7109031677\n",
    "Minibatch accuracy: 87.5\n",
    "Validation accuracy: 48.3\n",
    "Minibatch loss at step 3500: 7.69610118866\n",
    "Minibatch accuracy: 93.75\n",
    "Validation accuracy: 35.44\n",
    "Minibatch loss at step 3600: 34.2297134399\n",
    "Minibatch accuracy: 74.21875\n",
    "Validation accuracy: 49.24\n",
    "Minibatch loss at step 3700: 23.1608200073\n",
    "Minibatch accuracy: 83.59375\n",
    "Validation accuracy: 45.06\n",
    "Minibatch loss at step 3800: 7.34627771378\n",
    "Minibatch accuracy: 92.1875\n",
    "Validation accuracy: 45.04\n",
    "Minibatch loss at step 3900: 3.71520495415\n",
    "Minibatch accuracy: 95.3125\n",
    "Validation accuracy: 48.22\n",
    "Minibatch loss at step 4000: 28.8767375946\n",
    "Minibatch accuracy: 78.125\n",
    "Validation accuracy: 51.6\n",
    "Test accuracy: 58.28\n",
    "\n",
    "# Test 3\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 4001\n",
    "learning_rate = 0.0001\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "\n",
    "Graph Variables Initialized!\n",
    "Minibatch loss at step 0: 203.48550415\n",
    "Minibatch accuracy: 17.1875\n",
    "Validation accuracy: 8.66\n",
    "Minibatch loss at step 500: 314.713500977\n",
    "Minibatch accuracy: 3.125\n",
    "Validation accuracy: 18.22\n",
    "Minibatch loss at step 1000: 32.2654914856\n",
    "Minibatch accuracy: 71.09375\n",
    "Validation accuracy: 24.98\n",
    "Minibatch loss at step 1500: 150.726287842\n",
    "Minibatch accuracy: 13.28125\n",
    "Validation accuracy: 30.76\n",
    "Minibatch loss at step 2000: 59.0500297546\n",
    "Minibatch accuracy: 63.28125\n",
    "Validation accuracy: 39.84\n",
    "Minibatch loss at step 2500: 99.9961471558\n",
    "Minibatch accuracy: 41.40625\n",
    "Validation accuracy: 44.94\n",
    "Minibatch loss at step 3000: 94.2091064453\n",
    "Minibatch accuracy: 53.90625\n",
    "Validation accuracy: 47.42\n",
    "Minibatch loss at step 3500: 62.1564483643\n",
    "Minibatch accuracy: 62.5\n",
    "Validation accuracy: 50.98\n",
    "Minibatch loss at step 4000: 72.7012023926\n",
    "Minibatch accuracy: 54.6875\n",
    "Validation accuracy: 55.14\n",
    "Test accuracy: 60.94\n",
    "\n",
    "# Test 4\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 6001\n",
    "learning_rate = 0.0001\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "\n",
    "\n",
    "Graph Variables Initialized!\n",
    "Minibatch loss at step 0: 506.006622314\n",
    "Minibatch accuracy: 1.5625\n",
    "Validation accuracy: 8.78\n",
    "Minibatch loss at step 500: 311.409423828\n",
    "Minibatch accuracy: 7.03125\n",
    "Validation accuracy: 16.62\n",
    "Minibatch loss at step 1000: 36.9946594238\n",
    "Minibatch accuracy: 67.1875\n",
    "Validation accuracy: 25.12\n",
    "Minibatch loss at step 1500: 134.231628418\n",
    "Minibatch accuracy: 27.34375\n",
    "Validation accuracy: 32.5\n",
    "Minibatch loss at step 2000: 55.6944351196\n",
    "Minibatch accuracy: 61.71875\n",
    "Validation accuracy: 36.5\n",
    "Minibatch loss at step 2500: 125.744949341\n",
    "Minibatch accuracy: 21.09375\n",
    "Validation accuracy: 43.9\n",
    "Minibatch loss at step 3000: 81.8246002197\n",
    "Minibatch accuracy: 59.375\n",
    "Validation accuracy: 47.64\n",
    "Minibatch loss at step 3500: 45.8662719727\n",
    "Minibatch accuracy: 66.40625\n",
    "Validation accuracy: 48.54\n",
    "Minibatch loss at step 4000: 81.9520874023\n",
    "Minibatch accuracy: 50.78125\n",
    "Validation accuracy: 54.76\n",
    "Minibatch loss at step 4500: 42.9420814514\n",
    "Minibatch accuracy: 66.40625\n",
    "Validation accuracy: 53.74\n",
    "Minibatch loss at step 5000: 86.2162246704\n",
    "Minibatch accuracy: 45.3125\n",
    "Validation accuracy: 56.96\n",
    "Minibatch loss at step 5500: 52.2057647705\n",
    "Minibatch accuracy: 63.28125\n",
    "Validation accuracy: 60.0\n",
    "Minibatch loss at step 6000: 51.0174026489\n",
    "Minibatch accuracy: 66.40625\n",
    "Validation accuracy: 58.26\n",
    "Test accuracy: 65.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 6001\n",
    "learning_rate = 0.0001\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 506.006622314\n",
      "Minibatch accuracy: 1.5625\n",
      "Validation accuracy: 8.78\n",
      "Minibatch loss at step 500: 311.409423828\n",
      "Minibatch accuracy: 7.03125\n",
      "Validation accuracy: 16.62\n",
      "Minibatch loss at step 1000: 36.9946594238\n",
      "Minibatch accuracy: 67.1875\n",
      "Validation accuracy: 25.12\n",
      "Minibatch loss at step 1500: 134.231628418\n",
      "Minibatch accuracy: 27.34375\n",
      "Validation accuracy: 32.5\n",
      "Minibatch loss at step 2000: 55.6944351196\n",
      "Minibatch accuracy: 61.71875\n",
      "Validation accuracy: 36.5\n",
      "Minibatch loss at step 2500: 125.744949341\n",
      "Minibatch accuracy: 21.09375\n",
      "Validation accuracy: 43.9\n",
      "Minibatch loss at step 3000: 81.8246002197\n",
      "Minibatch accuracy: 59.375\n",
      "Validation accuracy: 47.64\n",
      "Minibatch loss at step 3500: 45.8662719727\n",
      "Minibatch accuracy: 66.40625\n",
      "Validation accuracy: 48.54\n",
      "Minibatch loss at step 4000: 81.9520874023\n",
      "Minibatch accuracy: 50.78125\n",
      "Validation accuracy: 54.76\n",
      "Minibatch loss at step 4500: 42.9420814514\n",
      "Minibatch accuracy: 66.40625\n",
      "Validation accuracy: 53.74\n",
      "Minibatch loss at step 5000: 86.2162246704\n",
      "Minibatch accuracy: 45.3125\n",
      "Validation accuracy: 56.96\n",
      "Minibatch loss at step 5500: 52.2057647705\n",
      "Minibatch accuracy: 63.28125\n",
      "Validation accuracy: 60.0\n",
      "Minibatch loss at step 6000: 51.0174026489\n",
      "Minibatch accuracy: 66.40625\n",
      "Validation accuracy: 58.26\n",
      "Test accuracy: 65.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4be3a247d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

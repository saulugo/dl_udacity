{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "# Tech Talk - Synergic Partners - Feb 23rd 2017\n",
    "\n",
    "# Sa√∫l Lugo\n",
    "\n",
    "#### This notebook has been adapted from an assigment of the introductory course to Deep Learning at www.udacity.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 28, 28) (100000,)\n",
      "Validation set (5000, 28, 28) (5000,)\n",
      "Test set (5000, 28, 28) (5000,)\n",
      "Final Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  final_test_dataset = save['final_test_dataset']\n",
    "  final_test_labels = save['final_test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "  print('Final Test set', final_test_dataset.shape, final_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 784) (100000, 10)\n",
      "Validation set (5000, 784) (5000, 10)\n",
      "Test set (5000, 784) (5000, 10)\n",
      "Final Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "final_test_dataset, final_test_labels = reformat(final_test_dataset, final_test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Final Test set', final_test_dataset.shape,final_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's introduce L2 regularization in the SGD logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "\n",
    "def log_reg(x, weights, biases):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: tensor containing the weights\n",
    "    biases: tensor containing the biases \n",
    "    ''' \n",
    "    return tf.add(tf.matmul(x,weights),biases)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.5\n",
    "num_steps = 3001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = log_reg(tf_train_dataset, weights, biases)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.200386\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 500: 1.715070\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 1000: 1.680272\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1500: 0.815077\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 2000: 1.050925\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 2500: 1.091488\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 3000: 0.988056\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.0%\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.5\n",
    "num_steps = 3001\n",
    "l2 = 0.002\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = log_reg(tf_train_dataset, weights, biases)\n",
    "  loss = tf.reduce_mean(tf.add(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels),l2*tf.nn.l2_loss(weights)))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.158524\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 8.7%\n",
      "Minibatch loss at step 500: 2.680648\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1000: 1.424516\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1500: 0.787888\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 0.765174\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2500: 0.798362\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 3000: 0.717879\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network Model Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First lets define a fucntion to represent the topology of our Neuronal Network:\n",
    "#Topology: Multilayer Perceptron, 1 hidden layer with 1024 neurons and RELU activation function.\n",
    "\n",
    "def mlp(x, weights, biases,l2=0):\n",
    "    '''\n",
    "    x: tf array with the training examples\n",
    "    weights: dictionary with the tensors containing the weights for each layer\n",
    "    biases: dictionary with the tensors containing the biases for each layer\n",
    "    '''\n",
    "    if(l2==0):\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "    else:\n",
    "        #h1 layer z = XW + b\n",
    "        h1_layer = tf.add(tf.matmul(x,weights['h1']), biases['b1'])\n",
    "        #h1 layer activation function relu(z)\n",
    "        h1_layer = tf.nn.relu(h1_layer)\n",
    "        #output layer (no activation needed after output layer)\n",
    "        out_layer = tf.add(tf.matmul(h1_layer,weights['out']), biases['out'])\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['h1']))\n",
    "        out_layer = tf.add(out_layer,l2*tf.nn.l2_loss(weights['out']))\n",
    "    \n",
    "    #we return the values predicted by the network in the output layer\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 316.791687012\n",
      "Minibatch accuracy: 10.15625\n",
      "Validation accuracy: 28.94\n",
      "Minibatch loss at step 500: 17.3185119629\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.82\n",
      "Minibatch loss at step 1000: 15.0953474045\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 78.84\n",
      "Minibatch loss at step 1500: 5.28393745422\n",
      "Minibatch accuracy: 82.8125\n",
      "Validation accuracy: 81.52\n",
      "Minibatch loss at step 2000: 2.11602020264\n",
      "Minibatch accuracy: 91.40625\n",
      "Validation accuracy: 80.76\n",
      "Minibatch loss at step 2500: 11.7564563751\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 79.98\n",
      "Minibatch loss at step 3000: 3.33054542542\n",
      "Minibatch accuracy: 80.46875\n",
      "Validation accuracy: 81.0\n",
      "Test accuracy: 87.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmqGoOFJEmqGoOFJEmqmv8fo0vni0F2r6wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe768159b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network Model With L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 3001\n",
    "learning_rate = 0.5\n",
    "display_step = 500\n",
    "n_hidden_1 = 1024\n",
    "n_imput = image_size * image_size\n",
    "n_classes = num_labels\n",
    "l2 = 0.0001\n",
    "\n",
    "#Neuronal network definitio as a TF Graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #graph imputs\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, n_imput))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, n_classes))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    #graph variables\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([n_imput, n_hidden_1])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_1,n_classes]))\n",
    "    }\n",
    "    \n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "    \n",
    "    #Network Topology: Fully connected 1 hidden 1024 neurons, relu activation function.\n",
    "    \n",
    "    net_out = mlp(tf_train_dataset, weights, biases,l2)\n",
    "    \n",
    "    #now we define the cost (loss) function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(net_out,tf_train_labels))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #new we define out optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    #now we define the prediction operations for training, validation and test data.\n",
    "    train_prediction = tf.nn.softmax(net_out)\n",
    "    valid_prediction = tf.nn.softmax(mlp(tf_valid_dataset,weights,biases))\n",
    "    test_prediction = tf.nn.softmax(mlp(tf_test_dataset,weights,biases))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Variables Initialized!\n",
      "Minibatch loss at step 0: 361.283630371\n",
      "Minibatch accuracy: 10.9375\n",
      "Validation accuracy: 27.26\n",
      "Minibatch loss at step 500: 12.817237854\n",
      "Minibatch accuracy: 85.15625\n",
      "Validation accuracy: 80.6\n",
      "Minibatch loss at step 1000: 9.0540933609\n",
      "Minibatch accuracy: 81.25\n",
      "Validation accuracy: 81.42\n",
      "Minibatch loss at step 1500: 8.4862985611\n",
      "Minibatch accuracy: 79.6875\n",
      "Validation accuracy: 77.94\n",
      "Minibatch loss at step 2000: 5.58230495453\n",
      "Minibatch accuracy: 82.03125\n",
      "Validation accuracy: 81.8\n",
      "Minibatch loss at step 2500: 6.35356235504\n",
      "Minibatch accuracy: 87.5\n",
      "Validation accuracy: 82.0\n",
      "Minibatch loss at step 3000: 1.86424708366\n",
      "Minibatch accuracy: 87.5\n",
      "Validation accuracy: 82.32\n",
      "Test accuracy: 88.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD8w3JiYddxC9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75642d9510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#let't store the cost at each epoch in order to plot it at the end\n",
    "\n",
    "mycost = []\n",
    "epoch = []\n",
    "\n",
    "#we compute the graph now!\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    #initializing the graph variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Graph Variables Initialized!\")\n",
    "    \n",
    "    for step in range(training_epochs):\n",
    "        #calculate the offset for the mini-batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        #generate the mini-batch\n",
    "        batch_data = train_dataset[offset:(offset+batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset+batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        \n",
    "        #running the graph with the mini-batch data as the training dataset\n",
    "        dummy, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #printing partial results each display_steps times\n",
    "        if(step % display_step == 0):\n",
    "            mycost.append(l)\n",
    "            epoch.append(step)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step,l))\n",
    "            print(\"Minibatch accuracy: {}\".format(accuracy(predictions,batch_labels)))\n",
    "            print(\"Validation accuracy: {}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {}\".format(accuracy(test_prediction.eval(),test_labels)))\n",
    "    \n",
    "    plt.plot(epoch,mycost,linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
